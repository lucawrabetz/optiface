from datetime import datetime
from typing import Any

from pathlib import Path

from pandas import DataFrame
from sqlalchemy import Engine, create_engine, engine
from sqlalchemy import (
    MetaData,
    Table,
    Column,
    Integer,
    String,
    Float,
    DateTime,
    Boolean,
    Inspector,
    inspect,
    insert,
)

from optiface.core.optispace import (
    ProblemSpace,
    Feature,
    run_key_features,
)

from optiface.core.optierror import Status, StatusOr, Failure, Success

from optiface.core.featuredata import _RUN_KEY_FDATA

from optiface.core.optidatetime import OptiDateTimeFactory

from optiface.constants import _SQLITE_PREF, _SPACE, _EXPERIMENTS_DBFILE

_RESULTS_TABLE_NAME = "results"

feature_to_alchemy_types: dict[type, type] = {
    str: String,
    int: Integer,
    float: Float,
    bool: Boolean,
    datetime: DateTime,
}


class AlchemyWAPI:
    def __init__(self, pspace: ProblemSpace, engine: Engine, metadata: MetaData):
        self.pspace: ProblemSpace = pspace
        self.odtf = OptiDateTimeFactory()
        self.engine: Engine = engine
        self.metadata: MetaData = metadata

    def insert_single_row(self, row: dict[str, Any]) -> None:
        stmt = insert(self.metadata.tables[_RESULTS_TABLE_NAME]).values(**row)
        stmt.compile()

        with self.engine.connect() as conn:
            conn.execute(stmt)
            conn.commit()

    def insert_rows(self, df: DataFrame) -> Status:
        status: Status = Success(title="Batch row insertion from AlchemyWAPI")

        for _, csv_row in df.iterrows():
            row = dict(csv_row)
            res: Status = self.pspace.validate_row(row)

            # validate - does not validate the run key (run_id, time_added, added_from), this is generated by us
            # TODO: need to improve multi-call-stack-level error msg propagation than just adding notes like this.
            if res.is_err():
                status.add_note(
                    note=f"Skipping non-valid row: {row}, with the following errors:",
                    file=__file__,
                )
                for errs in res.unwrap_err().values():
                    for e in errs:
                        status.add_note(note=e, file=__file__)
            else:
                self.pspace.add_run_key(row)
                self.insert_single_row(row)
                res.add_note(
                    note=f"added row {row} to results table in problem {self.pspace.name}",
                    file=__file__,
                )

        return status


class AlchemyFactory:
    def __init__(self, pspace: ProblemSpace):
        self.pspace: ProblemSpace = pspace
        self.dbpath: Path = Path(_SPACE) / pspace.name / _EXPERIMENTS_DBFILE

        # create_engine does not create db file if it DNE
        self.engine: Engine = create_engine(_SQLITE_PREF + str(self.dbpath), echo=True)
        # inspecting creates db file if it DNE
        self.inspector: Inspector = inspect(self.engine)

    def check_and_init_db(self) -> StatusOr[AlchemyWAPI]:
        tables: list[str] = self.inspector.get_table_names()
        failure: Failure = Failure(title="DB initialization")

        # other unknown tables in database
        if len(tables) > 1 or (len(tables) == 1 and tables[0] != _RESULTS_TABLE_NAME):
            failure.add_err(
                err=f"I cannot reconcile the problemspace {self.pspace.name} with its database, because there are unknown tables in the database: {', '.join(tables)}",
                file=__file__,
            )

        # no results table
        if not failure.has_errs and len(tables) == 0:
            return Success(
                value=self.create_db(), title="DB initialization, created new db"
            )

        # tables are correct
        # validate (check columns against problemspace), (no migration yet, missing column will fail operation, alembic soon?)
        # if validation passes (no errors added) reflect to return success
        feature_names = [f for f in _RUN_KEY_FDATA.keys()]
        feature_names.extend([f.name for f in self.pspace.full_row()])
        fnames = set(feature_names)

        if tables == [_RESULTS_TABLE_NAME]:
            # list[ReflectedColumn], which is effectively list[dict[str, str]]
            columns: list[Any] = self.inspector.get_columns(_RESULTS_TABLE_NAME)

            for col in columns:
                self._process_column(col=col, failure=failure, fnames=fnames)
                fnames.remove(col["name"])

        if len(fnames) > 0:
            failure.add_err(
                err=f"I cannot reconcile the problemspace {self.pspace.name} with its database: missing features {', '.join(fnames)}",
                file=__file__,
            )

        if failure.has_errs:
            return failure

        success = Success(value=self.reflect_db(), title="DB initialization, reflected")

        return success

    def _process_column(self, col, failure: Failure, fnames: set[str]) -> None:
        if col["name"] == "run_id" and col["primary_key"] == 0:
            failure.add_err(
                err=f"I cannot reconcile the problemspace {self.pspace.name} with its database: run_id must be primary_key",
                file=__file__,
            )
        if col["name"] not in fnames:
            failure.add_err(
                err=f"I cannot reconcile the problemspace {self.pspace.name} with its database: extra column {col['name']}",
                file=__file__,
            )

    def feature_to_column(self, feature: Feature, pk=False):
        # TODO: default is not actually writing to SQLAlchemy column object rn
        # does this even matter?
        return Column(
            feature.name,
            feature_to_alchemy_types[feature.feature_type],
            primary_key=pk,
            default=feature.default,
            nullable=not feature.required,
        )

    def run_key_columns(self) -> list[Column]:
        cols: list[Column] = []
        for feature_name, feature in run_key_features().items():
            pk = False
            if feature_name == "run_id":
                pk = True
            cols.append(self.feature_to_column(feature=feature, pk=pk))
        return cols

    def instance_key_columns(self) -> list[Column]:
        cols: list[Column] = []
        for _, feature in self.pspace.instance_key.items():
            pk = False
            cols.append(self.feature_to_column(feature=feature, pk=pk))
        return cols

    def solver_key_columns(self) -> list[Column]:
        cols: list[Column] = []
        for _, feature in self.pspace.solver_key.items():
            pk = False
            cols.append(self.feature_to_column(feature=feature, pk=pk))
        return cols

    def output_key_columns(self) -> list[Column]:
        cols: list[Column] = []
        for _, feature in self.pspace.output_key.items():
            pk = False
            cols.append(self.feature_to_column(feature=feature, pk=pk))
        return cols

    def create_db(self) -> AlchemyWAPI:
        columns: list[Column] = self.run_key_columns()
        columns.extend(self.instance_key_columns())
        columns.extend(self.solver_key_columns())
        columns.extend(self.output_key_columns())
        metadata = MetaData()
        self.results_table = Table(_RESULTS_TABLE_NAME, metadata, *columns)
        metadata.create_all(self.engine)
        return AlchemyWAPI(self.pspace, self.engine, metadata)

    def reflect_db(self) -> AlchemyWAPI:
        metadata = MetaData()
        self.results_table = Table(
            _RESULTS_TABLE_NAME, metadata, autoload_with=self.engine
        )
        return AlchemyWAPI(self.pspace, self.engine, metadata)


def init_alchemy_api(pspace: ProblemSpace) -> StatusOr[AlchemyWAPI]:
    af = AlchemyFactory(pspace)
    return af.check_and_init_db()


def main():
    print(__file__)
    from optiface.core.optispace import read_pspace_from_yaml
    import argparse

    parser = argparse.ArgumentParser(prog="test_dbm")
    parser.add_argument("problem", type=str)
    args = parser.parse_args()
    pspace_name = args.problem

    pspace = read_pspace_from_yaml(pspace_name)
    db_api = init_alchemy_api(pspace)

    if db_api:
        print(db_api.metadata)
        print(db_api.metadata.tables[_RESULTS_TABLE_NAME].columns)


if __name__ == "__main__":
    main()
